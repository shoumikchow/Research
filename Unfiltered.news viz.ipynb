{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import collections\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_disambiguation(name):\n",
    "    if (name == \"Khaleda\" or name == \"Zia\" or name == \"Begum Khaleda Zia\"):\n",
    "        return \"Khaleda Zia\"\n",
    "    if (name == \"Hasina\" or name == \"Sheikh\"):\n",
    "        return \"Sheikh Hasina\"\n",
    "    if (name == \"Fakhrul\"):\n",
    "        return \"Mirza Fakhrul Islam Alamgir\"\n",
    "    if (name == \"Muhith\" or name == \"AMA Muhith\" or name == \"MA Muhith\"):\n",
    "        return \"Abul Maal Abdul Muhith\"\n",
    "    if (name == \"Nizami\" or name == \"Motiur Rahman\"):\n",
    "        return \"Motiur Rahman Nizami\"\n",
    "    if (name == \"Modi\"):\n",
    "        return \"Narendra Modi\"\n",
    "    if (name == \"Bangabandhu\" or name == \"Sheikh Mujib\" or name == \"Sheikh Mujib\" or name == \"Bangabandhu Sheikh Mujibur\" or name == \"Sheikh Mujibur Rahman\"):\n",
    "        return \"Bangabandhu Sheikh Mujibur Rahman\"\n",
    "    if (name == \"Tarique\"):\n",
    "        return \"Tarique Rahman\"\n",
    "    if (name == \"Avijit\"):\n",
    "        return \"Avijit Roy\"\n",
    "    if (name == \"Mozena\"):\n",
    "        return \"Dan Mozena\"\n",
    "    if (name == \"Yunus\" or name == \"Mohammad Yunus\"):\n",
    "        return \"Muhammad Yunus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date limit: 2013-07-08 to 2016-06-14 on DT, New Age and Daily Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_json('Data/DT/bd_news_dt.json')\n",
    "dstar = pd.read_json('Data/DS/news_db.json', lines=True)\n",
    "dsun = pd.read_pickle('Data/Daily Sun/DailySun_ent_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_datetime(date_dict):\n",
    "    new_datetime = parser.parse(list(date_dict.items())[0][1], ignoretz=True)\n",
    "    new_datetime = new_datetime.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    return new_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['news_publish_date'] = dt['news_publish_date'].apply(conv_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar['date_published'] = dstar['date_published'].apply(conv_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun['date_published'] = pd.to_datetime(dsun['date_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2013, 7, 8)\n",
    "end_date = datetime(2016, 6, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_new = dt.loc[(dt['news_publish_date']>=start_date) & (dt['news_publish_date']<=end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar_new = dstar.loc[(dstar['date_published']>=start_date) & (dstar['date_published']<=end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun_new = dsun.loc[(dsun['date_published']>=start_date) & (dsun['date_published']<=end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: (46611, 17), dstar: (165236, 34), dsun(49055, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"dt: {}, dstar: {}, dsun{}\".format(dt_new.shape, dstar.shape, dt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_new = dt_new.reset_index(drop=True)\n",
    "dstar_new = dstar_new.reset_index(drop=True)\n",
    "dsun_new = dsun_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun_counts = dsun_new.groupby('date_published').sum().agg({\n",
    "    'location_entities': collections.Counter, \n",
    "    'organization_entities': collections.Counter,\n",
    "    'person_entities': collections.Counter\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun_counts['location_entities'] = dsun_counts['location_entities'].apply(lambda x: x.most_common())\n",
    "dsun_counts['organization_entities'] = dsun_counts['organization_entities'].apply(lambda x: x.most_common())\n",
    "dsun_counts['person_entities'] = dsun_counts['person_entities'].apply(lambda x: x.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Star and DT doesn't work the same way for some absurd reason.\n",
    "# Trying on a small dataset\n",
    "\n",
    "# df = pd.read_json('Data/DS/test.json', lines=True)\n",
    "# df['date_published'] = df['date_published'].apply(conv_to_datetime)\n",
    "# df_counts = df.groupby('date_published').sum().agg({\n",
    "#     'ner_unique_location': collections.Counter, \n",
    "#     'ner_unique_organization': collections.Counter,\n",
    "#     'ner_unique_person': collections.Counter\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar_counts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar_counts['location_entities'] = dstar_new.groupby('date_published')['ner_unique_location'].sum().apply(collections.Counter, 1)\n",
    "dstar_counts['organization_entities'] = dstar_new.groupby('date_published')['ner_unique_organization'].sum().apply(collections.Counter, 1)\n",
    "dstar_counts['person_entities'] = dstar_new.groupby('date_published')['ner_unique_person'].sum().apply(collections.Counter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar_counts['location_entities'] = dstar_counts['location_entities'].apply(lambda x: x.most_common())\n",
    "dstar_counts['organization_entities'] = dstar_counts['organization_entities'].apply(lambda x: x.most_common())\n",
    "dstar_counts['person_entities'] = dstar_counts['person_entities'].apply(lambda x: x.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the news_ner_tags column, which contains dictionaries, into different columns based on dictionary keys\n",
    "dt_new_modified = pd.concat([dt_new.drop(['news_ner_tags'], axis=1), dt_new['news_ner_tags'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_counts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_counts['location_entities'] = dt_new_modified.groupby('news_publish_date')['locations_unique'].sum().apply(collections.Counter, 1)\n",
    "dt_counts['organization_entities'] = dt_new_modified.groupby('news_publish_date')['organizations_unique'].sum().apply(collections.Counter, 1)\n",
    "dt_counts['person_entities'] = dt_new_modified.groupby('news_publish_date')['persons_unique'].sum().apply(collections.Counter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_counts['location_entities'] = dt_counts['location_entities'].apply(lambda x: x.most_common())\n",
    "dt_counts['organization_entities'] = dt_counts['organization_entities'].apply(lambda x: x.most_common())\n",
    "dt_counts['person_entities'] = dt_counts['person_entities'].apply(lambda x: x.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Use TF-IDF on the NERs instead of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
