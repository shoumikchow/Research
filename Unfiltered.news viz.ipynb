{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_disambiguation(name):\n",
    "    if (name == \"Khaleda\" or name == \"Zia\" or name == \"Begum Khaleda Zia\"):\n",
    "        return \"Khaleda Zia\"\n",
    "    if (name == \"Hasina\" or name == \"Sheikh\"):\n",
    "        return \"Sheikh Hasina\"\n",
    "    if (name == \"Fakhrul\"):\n",
    "        return \"Mirza Fakhrul Islam Alamgir\"\n",
    "    if (name == \"Muhith\" or name == \"AMA Muhith\" or name == \"MA Muhith\"):\n",
    "        return \"Abul Maal Abdul Muhith\"\n",
    "    if (name == \"Nizami\" or name == \"Motiur Rahman\"):\n",
    "        return \"Motiur Rahman Nizami\"\n",
    "    if (name == \"Modi\"):\n",
    "        return \"Narendra Modi\"\n",
    "    if (name == \"Bangabandhu\" or name == \"Sheikh Mujib\" or name == \"Sheikh Mujib\" or name == \"Bangabandhu Sheikh Mujibur\" or name == \"Sheikh Mujibur Rahman\"):\n",
    "        return \"Bangabandhu Sheikh Mujibur Rahman\"\n",
    "    if (name == \"Tarique\"):\n",
    "        return \"Tarique Rahman\"\n",
    "    if (name == \"Avijit\"):\n",
    "        return \"Avijit Roy\"\n",
    "    if (name == \"Mozena\"):\n",
    "        return \"Dan Mozena\"\n",
    "    if (name == \"Yunus\" or name == \"Mohammad Yunus\"):\n",
    "        return \"Muhammad Yunus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date limit: 2013-07-08 to 2016-06-14 on DT, Daily Star and Daily Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_json('Data/DT/bd_news_dt.json')\n",
    "dstar = pd.read_json('Data/DS/news_db.json', lines=True)\n",
    "dsun = pd.read_pickle('Data/Daily Sun/DailySun_ent_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_datetime(date_dict):\n",
    "    new_datetime = parser.parse(list(date_dict.items())[0][1], ignoretz=True)\n",
    "    new_datetime = new_datetime.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    return new_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bangladesh', 'politics', 'law and rights', 'safety', 'labour', 'development', 'education', 'environment', 'foreign affairs', 'crime', 'agriculture', 'science'}\n"
     ]
    }
   ],
   "source": [
    "dt_tags = list(dt['news_original_tags'])\n",
    "dt_tags = [item for sublist in dt_tags for item in sublist]\n",
    "print(set(dt_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/back-page', '/my-districts', '/winner', '/culturetainment', '/metropolis', '/our-faith', '/news-link', '/asia-print', '/editorial', '/business-print', '/world-print', '/front-page'}\n"
     ]
    }
   ],
   "source": [
    "dsun_tags = set(list(dsun['section']))\n",
    "print(dsun_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Star Chittagong', 'National', 'World', 'Opinion', 'Showbiz', 'Letters', 'The Star', 'Lifestyle', 'Health', 'Sports', 'Bytes', 'Star Weekend', 'Next Step', 'In Focus', 'Wide Angle', 'Editorial', 'Law & Our Rights', 'Arts & Entertainment', 'Metropolitan', 'Shout', 'Shift', 'Star City', 'Back Page', 'Front Page', 'Strategic Issues', 'Country', 'Star People', 'Business', 'City', 'Literature', 'Book Reviews'}\n"
     ]
    }
   ],
   "source": [
    "dstar_tags = set(list(dstar['section']))\n",
    "print(dstar_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['news_publish_date'] = dt['news_publish_date'].apply(conv_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar['date_published'] = dstar['date_published'].apply(conv_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun['date_published'] = pd.to_datetime(dsun['date_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2013, 7, 8)\n",
    "end_date = datetime(2016, 6, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_new = dt.loc[(dt['news_publish_date']>=start_date) & (dt['news_publish_date']<=end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar_new = dstar.loc[(dstar['date_published']>=start_date) & (dstar['date_published']<=end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun_new = dsun.loc[(dsun['date_published']>=start_date) & (dsun['date_published']<=end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt: (46611, 17), dstar: (165236, 34), dsun(49055, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"dt: {}, dstar: {}, dsun{}\".format(dt_new.shape, dstar.shape, dt.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_new = dt_new.reset_index(drop=True)\n",
    "dstar_new = dstar_new.reset_index(drop=True)\n",
    "dsun_new = dsun_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun_counts = dsun_new.groupby('date_published').sum().agg({\n",
    "    'location_entities': collections.Counter, \n",
    "    'organization_entities': collections.Counter,\n",
    "    'person_entities': collections.Counter\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts Counter object\n",
    "\n",
    "dsun_counts['location_entities'] = dsun_counts['location_entities'].apply(lambda x: x.most_common())\n",
    "dsun_counts['organization_entities'] = dsun_counts['organization_entities'].apply(lambda x: x.most_common())\n",
    "dsun_counts['person_entities'] = dsun_counts['person_entities'].apply(lambda x: x.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Star and DT doesn't work the same way for some absurd reason.\n",
    "# Trying on a small dataset\n",
    "\n",
    "# df = pd.read_json('Data/DS/test.json', lines=True)\n",
    "# df['date_published'] = df['date_published'].apply(conv_to_datetime)\n",
    "# df_counts = df.groupby('date_published').sum().agg({\n",
    "#     'ner_unique_location': collections.Counter, \n",
    "#     'ner_unique_organization': collections.Counter,\n",
    "#     'ner_unique_person': collections.Counter\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar_counts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstar_counts['location_entities'] = dstar_new.groupby('date_published')['ner_unique_location'].sum().apply(collections.Counter, 1)\n",
    "dstar_counts['organization_entities'] = dstar_new.groupby('date_published')['ner_unique_organization'].sum().apply(collections.Counter, 1)\n",
    "dstar_counts['person_entities'] = dstar_new.groupby('date_published')['ner_unique_person'].sum().apply(collections.Counter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts Counter object\n",
    "\n",
    "\n",
    "dstar_counts['location_entities'] = dstar_counts['location_entities'].apply(lambda x: x.most_common())\n",
    "dstar_counts['organization_entities'] = dstar_counts['organization_entities'].apply(lambda x: x.most_common())\n",
    "dstar_counts['person_entities'] = dstar_counts['person_entities'].apply(lambda x: x.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the news_ner_tags column, which contains dictionaries, into different columns based on dictionary keys\n",
    "dt_new_modified = pd.concat([dt_new.drop(['news_ner_tags'], axis=1), dt_new['news_ner_tags'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_counts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_counts['location_entities'] = dt_new_modified.groupby('news_publish_date')['locations_unique'].sum().apply(collections.Counter, 1)\n",
    "dt_counts['organization_entities'] = dt_new_modified.groupby('news_publish_date')['organizations_unique'].sum().apply(collections.Counter, 1)\n",
    "dt_counts['person_entities'] = dt_new_modified.groupby('news_publish_date')['persons_unique'].sum().apply(collections.Counter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts Counter object\n",
    "\n",
    "# dt_counts['location_entities'] = dt_counts['location_entities'].apply(lambda x: x.most_common())\n",
    "# dt_counts['organization_entities'] = dt_counts['organization_entities'].apply(lambda x: x.most_common())\n",
    "# dt_counts['person_entities'] = dt_counts['person_entities'].apply(lambda x: x.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_counts.to_json('Data/DT/dt_counts.json')\n",
    "# dstar_counts.to_json('Data/DS/dstar_counts.json')\n",
    "# dsun_counts.to_json('Data/Daily Sun/dsun_counts.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun_tfidf = pd.DataFrame()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsun_new['location_entities'] =  dsun_new['location_entities'].progress_apply(lambda x: ' '.join(str(e) for e in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsun_tfidf['location_entities'] = dsun_new.groupby('date_published').progress_apply(tfidf.fit_transform(dsun_new['location_entities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Date': ['2018-09-01','2018-09-01 ','2018-09-02'],\n",
    "               'documents': [['Dhaka', 'Chittagong', 'Sylhet'],['Dhaka', 'India'],['Dhaka']]})\n",
    "\n",
    "\n",
    "df['documents'] = df['documents'].apply(lambda x: ' '.join(str(e) for e in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Dhaka Chittagong Sylhet\n",
       "1                Dhaka India\n",
       "2                      Dhaka\n",
       "Name: documents, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'csr_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3c6413228c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'documents'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'documents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# x = v.fit_transform(df['documents'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# df1 = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_builtin_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;31m# this is needed so we don't try and wrap strings. If we could\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_is_builtin_func\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0motherwise\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \"\"\"\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_builtin_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'csr_matrix'"
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer()\n",
    "\n",
    "df['documents'] = df.groupby('Date').apply(v.fit_transform(df['documents']))\n",
    "# x = v.fit_transform(df['documents'])\n",
    "# df1 = pd.DataFrame(x.toarray(), columns=v.get_feature_names())\n",
    "# res = pd.concat([df, df1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
