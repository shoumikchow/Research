{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy\n",
    "from unidecode import unidecode\n",
    "import csv\n",
    "# import nltk.data\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "# tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsun = pd.read_pickle('Data/Daily Sun/DailySun_ent_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_smart_punctuations(text):\n",
    "    return unidecode(text)\n",
    "#     text.replace(\"‘\", \"\\'\").replace(\"’\", \"\\'\").replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"–\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_smart_from_contracted(text):\n",
    "    new_text = text.replace(\"’m\", \"'m\").replace(\"’s\", \"'s\").replace(\"’re\", \"'re\").replace(\"’ve\", \"'ve\").replace(\"’t\", \"'t\").replace(\"’d\", \"'d\").replace(\"’ll\", \"'ll\")\n",
    "    new_text = new_text.replace(\"’\", \"\\\"\").replace(\"‘\", \"\\\"\")\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alleged(text):\n",
    "    if re.search(r'\\balleged\\b', text):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "conversational_words = ['said', 'told', 'asked', 'speak', 'say', 'tell', 'spoke', 'added', 'declare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_quotes(line):\n",
    "    line = line\n",
    "\n",
    "    if \"\\n\" in line:\n",
    "        arr = line.split(\"\\n\")\n",
    "        i = 0\n",
    "        prev_sentence_has_double_quote = False\n",
    "        while i < len(arr):\n",
    "            if arr[i] == \"\":\n",
    "                i += 2  # Each of the paragraphs are separated by two new lines so i is incremented by 2. \n",
    "                continue\n",
    "\n",
    "            if arr[i].count(\"\\\"\") == 1: # If a paragraph has a line like ~ ABC said, 'DEF is good. ~ it means the quotation continues in the next paragraph since it exists in pairs and the next double quote will be found in the next paragraph\n",
    "                if prev_sentence_has_double_quote is False:\n",
    "                    prev_sentence_has_double_quote = True\n",
    "                    i += 2\n",
    "                    continue\n",
    "\n",
    "            if arr[i][0] == '\"': # Matching pair of the quotation mark found\n",
    "                if prev_sentence_has_double_quote is True:\n",
    "                    s = list(arr[i])\n",
    "                    s[0] = ' '\n",
    "                    arr[i] = \"\".join(s)\n",
    "                    arr[i - 2:i + 1] = [''.join(arr[i - 2:i + 1])]  # Concatenating the two paragraphs.\n",
    "                    prev_sentence_has_double_quote = False\n",
    "                    i += 2\n",
    "                    continue\n",
    "\n",
    "            if prev_sentence_has_double_quote is True:\n",
    "                prev_sentence_has_double_quote = False\n",
    "            i += 2\n",
    "\n",
    "        for i in reversed(arr):\n",
    "            if i == \" \" or i == '':\n",
    "                arr.pop()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Final output being placed in str\n",
    "        final_str = \"\"\n",
    "        for i in range(0, len(arr)):\n",
    "            if(arr[i] == ' ' or arr[i] == ''):\n",
    "                final_str += \"\\n\" + \"\\n\"\n",
    "            else:\n",
    "                final_str += arr[i]\n",
    "\n",
    "        return(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_space_after_dot(text):\n",
    "    rx = r\"\\.(?=\\S)\"\n",
    "    fixed = re.sub(rx, \". \", text)\n",
    "    return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_multiple_space(text):\n",
    "  fixed = re.sub(' +',' ', text)\n",
    "  return fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quotes(text):\n",
    "\n",
    "    regx = re.compile('([^\\.\\?\\!]*?\".+?\".*?[\\.\\?\\!])|([^\\.\\?\\!]*?\".+?[\\.\\?\\!]\")')\n",
    "\n",
    "    array = (regx.findall(text))\n",
    "\n",
    "    quoted_sentence = []\n",
    "    for elem in array:\n",
    "        if elem[0] == '':\n",
    "            quoted_sentence.append(elem[1].strip())\n",
    "        elif elem[1] == '':\n",
    "            quoted_sentence.append(elem[0].strip())\n",
    "\n",
    "    return quoted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_in_quote(content, locations, organizations, persons):\n",
    "    loc, org, per, entities = [], [], [], []\n",
    "    if any(entity in content for entity in locations) or any(entity in content for entity in organizations) or any(entity in content for entity in persons):\n",
    "        \n",
    "        for entity in locations:\n",
    "            if entity in content:\n",
    "                loc.append(entity)\n",
    "                \n",
    "        for entity in organizations:\n",
    "            if entity in content:\n",
    "                org.append(entity)\n",
    "                \n",
    "        for entity in persons:\n",
    "            if entity in content:\n",
    "                per.append(entity)\n",
    "\n",
    "    entities = [loc, org, per]\n",
    "    if entities:\n",
    "        return content, loc, org, per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328748, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsun.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column names in source:\n",
    "\n",
    "\n",
    "['category', 'date_published', 'image', 'news_content', 'news_id',\n",
    "       'newspaper', 'reporter', 'tags', 'title', 'url', 'location_entities',\n",
    "       'organization_entities', 'person_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns=['_id', 'timestamp', 'quote', 'locations', 'organizations', 'persons', 'headline', 'keywords', 'category'])\n",
    "\n",
    "# # counter = 0\n",
    "# for index, row in tqdm(dsun.iterrows(), total=dsun.shape[0]):\n",
    "\n",
    "# #     counter += 1\n",
    "# #     if counter == 100:\n",
    "# #         break\n",
    "#     try:\n",
    "#         _id = row['news_id']\n",
    "#         timestamp = row['date_published']\n",
    "#         content = row['news_content']\n",
    "#         locations = row['location_entities']\n",
    "#         organizations = row['organization_entities']\n",
    "#         persons = row['person_entities']\n",
    "#         headline = row['title']\n",
    "#         keywords = row['tags']\n",
    "#         category = row['category']\n",
    "#         if content:\n",
    "#             # content = fix_quotes(remove_smart_punctuations(content)) # fixes the problem with non-matching quotation marks due to continutation in next para\n",
    "#             # content = fix_space_after_dot(content)\n",
    "#             content = remove_smart_punctuations(remove_smart_from_contracted(content))\n",
    "#             quotes = find_quotes(content) # finds all the quotes in the content as a list\n",
    "\n",
    "#             # Checks for quotation marks\n",
    "#             for quote in quotes:\n",
    "#                 if quote:\n",
    "#                     text, loc, org, per = entities_in_quote(quote, locations, organizations, persons)\n",
    "\n",
    "#                 if (len(loc + org + per) >= 2):\n",
    "#                     df = df.append(pd.Series([_id, timestamp, text, loc, org, per, headline, keywords, category],\n",
    "#                                             index = ['_id', 'timestamp', 'quote', 'locations', 'organizations', 'persons', 'headline', 'keywords', 'category']),  ignore_index=True)\n",
    "\n",
    "#             # Checks for conversational words, ignoring \n",
    "#             doc = nlp(content)\n",
    "#             period_delimited_list = [sent.string.strip() for sent in doc.sents]\n",
    "\n",
    "#             for sentence in period_delimited_list:\n",
    "#                 if any(word in sentence for word in conversational_words) or check_alleged(sentence):\n",
    "#                     if quotes:\n",
    "#                         sentence, loc, org, per = entities_in_quote(\n",
    "#                                     sentence, locations, organizations, persons)\n",
    "#                         if not any(sentence in s for s in quotes) and len(loc + org + per) >= 2:\n",
    "#                                 df = df.append(pd.Series([_id, timestamp, sentence, loc, org, per, headline, keywords, category],\n",
    "#                                             index = ['_id', 'timestamp', 'quote', 'locations', 'organizations', 'persons', 'headline', 'keywords', 'category']),  ignore_index=True)\n",
    "#     except:\n",
    "#         print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 328748/328748 [11:43:10<00:00,  7.79it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('Data/Processed Data/Quotations/DailySun-quotations.tsv', 'w') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "    writer.writerow(['_id', 'timestamp', 'quote', 'locations', 'organizations', 'persons', 'section'])\n",
    "    \n",
    "\n",
    "# counter = 0\n",
    "for index, row in tqdm(dsun.iterrows(), total=dsun.shape[0]):\n",
    "    \n",
    "#     if counter == 1000:\n",
    "#         break\n",
    "#     counter += 1\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        _id = row['_id']\n",
    "        timestamp = row['date_published']\n",
    "        content = row['news_content']\n",
    "        locations = row['location_entities']\n",
    "        organizations = row['organization_entities']\n",
    "        persons = row['person_entities']\n",
    "        section = row['section']\n",
    "        if content:\n",
    "            # content = fix_quotes(remove_smart_punctuations(content)) # fixes the problem with non-matching quotation marks due to continutation in next para\n",
    "            content = fix_multiple_space(content).replace(\"\\\\\",\"\")\n",
    "            quotes = find_quotes(content) # finds all the quotes in the content as a list\n",
    "\n",
    "            # Checks for quotation marks\n",
    "            for quote in quotes:\n",
    "                if quote:\n",
    "                    text, loc, org, per = entities_in_quote(quote, locations, organizations, persons)\n",
    "\n",
    "                if (len(loc + org + per) >= 2):\n",
    "                  \n",
    "                  with open('Data/Processed Data/Quotations/DailySun-quotations.tsv', 'a') as outfile:\n",
    "                    writer = csv.writer(outfile, delimiter='\\t')\n",
    "                    writer.writerow([_id, timestamp, text, loc, org, per, section])\n",
    "                 \n",
    "                    \n",
    "            # Checks for conversational words, ignoring \n",
    "            doc = nlp(content)\n",
    "            period_delimited_list = [sent.string.strip() for sent in doc.sents]\n",
    "\n",
    "            for sentence in period_delimited_list:\n",
    "                if any(word in sentence for word in conversational_words) or check_alleged(sentence):\n",
    "                    if quotes:\n",
    "                        sentence, loc, org, per = entities_in_quote(\n",
    "                                    sentence, locations, organizations, persons)\n",
    "                        if not any(sentence in s for s in quotes) and len(loc + org + per) >= 2:\n",
    "                          \n",
    "\n",
    "                            with open('Data/Processed Data/Quotations/DailySun-quotations.tsv', 'a') as outfile:\n",
    "                                writer = csv.writer(outfile, delimiter='\\t')\n",
    "                                writer.writerow([_id, timestamp, sentence, loc, org, per, section])\n",
    "    except:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
